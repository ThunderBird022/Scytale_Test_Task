{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "github_token = os.getenv(\"git_token\")\n",
    "headers = {\n",
    "        'Authorization': f'token {github_token}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_github():\n",
    "    org_name = \"Scytale-exercise\"\n",
    "    api_base_url = \"https://api.github.com\"\n",
    "    output_folder = \"git_data\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    organization_info = get_org_info(api_base_url, github_token, org_name)\n",
    "    repositories_info = get_repositories(api_base_url, github_token, organization_info)\n",
    "    save_pull_requests_to_files(repositories_info, github_token, output_folder)\n",
    "\n",
    "\n",
    "def get_org_info(api_base_url, github_token, org_name):\n",
    "    organization_url = f\"{api_base_url}/orgs/{org_name}\"\n",
    "    \n",
    "    response = requests.get(organization_url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_repositories(api_base_url, github_token, org_info):\n",
    "    repositories_url = org_info[\"repos_url\"]\n",
    "\n",
    "    response = requests.get(repositories_url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def save_pull_requests_to_files(repositories_info, github_token, output_folder):\n",
    "    for repo in repositories_info:\n",
    "        repo_id = repo[\"id\"]\n",
    "        repo_name = repo[\"name\"]\n",
    "        repo_owner = repo[\"owner\"][\"login\"]\n",
    "\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            pull_requests_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/pulls?\" \\\n",
    "                                f\"state=all&page={page_number}\"\n",
    "\n",
    "            response = requests.get(pull_requests_url, headers=headers)\n",
    "            pull_requests_data = response.json()\n",
    "\n",
    "            if not pull_requests_data:\n",
    "                break\n",
    "\n",
    "            file_name = f\"{repo_name}_{repo_owner}_{page_number}_pulls.json\"\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "            with open(file_path, \"w\") as file:\n",
    "                json.dump(pull_requests_data, file)\n",
    "\n",
    "            page_number += 1\n",
    "\n",
    "\n",
    "fetch_data_from_github()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+----------------+--------------------+-------+--------------+------------+--------------------+\n",
      "|Organization Name|repository_id|repository_name|repository_owner|           merged_at|num_prs|num_prs_merged|is_compliant|            max_date|\n",
      "+-----------------+-------------+---------------+----------------+--------------------+-------+--------------+------------+--------------------+\n",
      "| Scytale-exercise|    724133322|   Scytale_repo|Scytale-exercise|2023-11-27T13:46:31Z|      2|             1|       false|2023-11-27T13:46:31Z|\n",
      "| Scytale-exercise|    724140378|  scytale-repo2|Scytale-exercise|2023-11-27T13:34:05Z|      1|             1|        true|2023-11-27T13:34:05Z|\n",
      "| Scytale-exercise|    721612130|  scytale-repo3|Scytale-exercise|2023-11-21T12:29:07Z|      4|             4|        true|2023-11-21T12:29:07Z|\n",
      "+-----------------+-------------+---------------+----------------+--------------------+-------+--------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "def load_json_files(directory, spark_session: SparkSession):\n",
    "    json_dataframes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        json_dataframes.append(spark_session.read.json(os.path.join(directory, filename), multiLine=True))\n",
    "    return json_dataframes\n",
    "\n",
    "\n",
    "def process_github_data(dataframe):\n",
    "    transformed_df = dataframe.select(\n",
    "        split(col(\"base.repo.full_name\"), '/').getItem(0).alias(\"Organization Name\"),\n",
    "        col(\"base.repo.id\").alias(\"repository_id\"),\n",
    "        col(\"base.repo.name\").alias(\"repository_name\"),\n",
    "        col(\"base.repo.owner.login\").alias(\"repository_owner\"),\n",
    "        col(\"closed_at\").alias(\"merged_at\")\n",
    "    ).filter(dataframe.closed_at.isNotNull())\n",
    "    \n",
    "    total_prs = dataframe.select(dataframe.base).count()\n",
    "    transformed_df = transformed_df.withColumn(\"num_prs\", lit(total_prs))\n",
    "    \n",
    "    total_merged_prs = dataframe.filter(dataframe.state.isin(['closed'])).count()\n",
    "    transformed_df = transformed_df.withColumn(\"num_prs_merged\", lit(total_merged_prs))\n",
    "    \n",
    "    transformed_df = transformed_df.withColumn(\n",
    "        \"is_compliant\",\n",
    "        (transformed_df.num_prs == transformed_df.num_prs_merged) & \n",
    "        (transformed_df.repository_owner.contains('Scytale'))\n",
    "    )\n",
    "    \n",
    "    latest_merge_date_df = dataframe.filter(col(\"closed_at\").isNotNull()) \\\n",
    "        .agg(max(col(\"closed_at\")).alias(\"max_date\"))\n",
    "    \n",
    "    transformed_df = transformed_df.join(\n",
    "        latest_merge_date_df, \n",
    "        transformed_df.merged_at == latest_merge_date_df.max_date, \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "def save_to_parquet(json_dataframes):\n",
    "    transformed_dataframes = [process_github_data(df) for df in json_dataframes]\n",
    "    final_dataframe = reduce(pyspark.sql.dataframe.DataFrame.unionByName, transformed_dataframes)\n",
    "    final_dataframe.show()\n",
    "    final_dataframe.write.parquet(\"./result.parquet\")\n",
    "\n",
    "\n",
    "# spark_context = SparkContext\n",
    "spark_session = SparkSession.builder.master(\"local\").appName(\"Spark\").getOrCreate()\n",
    "\n",
    "json_dataframes = load_json_files(\"./git_data\", spark_session)\n",
    "\n",
    "save_to_parquet(json_dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
